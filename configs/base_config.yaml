# Dataset and Tokenizer
dataset_name: "iwslt2017"
dataset_config: "iwslt2017-en-de"
src_lang: "en"
tgt_lang: "de"
vocab_size: 20000
tokenizer_path: "./tokenizer" # Directory to save/load tokenizers

# Model Hyperparameters 
d_model: 256          # Embedding dimension 
n_heads: 8            # Number of heads 
#n_heads: 1           # 用于消融实验
n_layers: 3           # Number of encoder/decoder layers 
d_ff: 512             # Feed-forward dimension
dropout: 0.1
max_seq_len: 128      # Maximum sequence length

# Training Settings
device: "cuda"
epochs: 15
batch_size: 64
learning_rate: 0.0003
optimizer: "AdamW"
weight_decay: 0.01
lr_warmup_steps: 4000
grad_clip_thresh: 1.0
label_smoothing: 0.1

# Reproducibility
seed: 42

# Logging and Saving
log_interval: 100
model_save_path: "./checkpoints/best_model.pt"
results_dir: "./results"